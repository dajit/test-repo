{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "hr-hack-submission-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajit/test-repo/blob/release1/hr_hack_submission_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_c_JgGIj6mq"
      },
      "source": [
        "# version history\n",
        "# ajit dhamale v2\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import zscore"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEeHgOp-j6mw"
      },
      "source": [
        "Data processing start from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL2mY0x5kETY",
        "outputId": "1061b0d9-4325-4aba-b684-1b4ce7681617"
      },
      "source": [
        "# Ajit Dhamale\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pD6dfZZkEdf",
        "outputId": "b5f65680-478c-4090-e84a-bfd13899dbbb"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/hackathon-hr')\n",
        "!pwd\n",
        "!ls "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/hackathon-hr\n",
            " hr-hack-submission-v1.ipynb   merged_orig_test_data.csv\n",
            " hr-hack-submission-v2.ipynb   prepare-test-data-v1.ipynb\n",
            " idf.csv\t\t       tdf.csv\n",
            " idf.zip\t\t       test-data\n",
            "'idf.zip (Unzipped Files)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqexaKnkElD"
      },
      "source": [
        "idf = pd.read_csv(\"idf.csv\")  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhIK660LnnE5",
        "outputId": "0bf0e304-a0d1-4382-fe67-701616dc96c0"
      },
      "source": [
        "idf.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 369289 entries, 0 to 369288\n",
            "Data columns (total 37 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   VLF                      369289 non-null  float64\n",
            " 1   VLF_PCT                  369289 non-null  float64\n",
            " 2   LF                       369289 non-null  float64\n",
            " 3   LF_PCT                   369289 non-null  float64\n",
            " 4   LF_NU                    369289 non-null  float64\n",
            " 5   HF                       369289 non-null  float64\n",
            " 6   HF_PCT                   369289 non-null  float64\n",
            " 7   HF_NU                    369289 non-null  float64\n",
            " 8   TP                       369289 non-null  float64\n",
            " 9   LF_HF                    369289 non-null  float64\n",
            " 10  HF_LF                    369289 non-null  float64\n",
            " 11  SD1                      369289 non-null  float64\n",
            " 12  SD2                      369289 non-null  float64\n",
            " 13  sampen                   369289 non-null  float64\n",
            " 14  higuci                   369289 non-null  float64\n",
            " 15  MEAN_RR                  369289 non-null  float64\n",
            " 16  MEDIAN_RR                369289 non-null  float64\n",
            " 17  SDRR                     369289 non-null  float64\n",
            " 18  RMSSD                    369289 non-null  float64\n",
            " 19  SDSD                     369289 non-null  float64\n",
            " 20  SDRR_RMSSD               369289 non-null  float64\n",
            " 21  HR                       369289 non-null  float64\n",
            " 22  pNN25                    369289 non-null  float64\n",
            " 23  pNN50                    369289 non-null  float64\n",
            " 24  KURT                     369289 non-null  float64\n",
            " 25  SKEW                     369289 non-null  float64\n",
            " 26  MEAN_REL_RR              369289 non-null  float64\n",
            " 27  MEDIAN_REL_RR            369289 non-null  float64\n",
            " 28  SDRR_REL_RR              369289 non-null  float64\n",
            " 29  RMSSD_REL_RR             369289 non-null  float64\n",
            " 30  SDSD_REL_RR              369289 non-null  float64\n",
            " 31  SDRR_RMSSD_REL_RR        369289 non-null  float64\n",
            " 32  KURT_REL_RR              369289 non-null  float64\n",
            " 33  SKEW_REL_RR              369289 non-null  float64\n",
            " 34  condition_interruption   369289 non-null  int64  \n",
            " 35  condition_no stress      369289 non-null  int64  \n",
            " 36  condition_time pressure  369289 non-null  int64  \n",
            "dtypes: float64(34), int64(3)\n",
            "memory usage: 104.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ4bDXAOj6mw"
      },
      "source": [
        "# Clean up  not required columns\n",
        "# merged_orig_df\n",
        "\n",
        "# drop uuid column and datasetid\n",
        "\n",
        "#idf = merged_orig_df.drop(['uuid','datasetId'], axis=1)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R0hpx_0j6mw",
        "outputId": "030460bb-6bd9-4d71-e82e-672b08ac4013"
      },
      "source": [
        "idf.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(369289, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EohllR3ej6mw",
        "outputId": "ead4e4af-0788-48b3-f461-5ea39e76741c"
      },
      "source": [
        "idf.info()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 369289 entries, 0 to 369288\n",
            "Data columns (total 37 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   VLF                      369289 non-null  float64\n",
            " 1   VLF_PCT                  369289 non-null  float64\n",
            " 2   LF                       369289 non-null  float64\n",
            " 3   LF_PCT                   369289 non-null  float64\n",
            " 4   LF_NU                    369289 non-null  float64\n",
            " 5   HF                       369289 non-null  float64\n",
            " 6   HF_PCT                   369289 non-null  float64\n",
            " 7   HF_NU                    369289 non-null  float64\n",
            " 8   TP                       369289 non-null  float64\n",
            " 9   LF_HF                    369289 non-null  float64\n",
            " 10  HF_LF                    369289 non-null  float64\n",
            " 11  SD1                      369289 non-null  float64\n",
            " 12  SD2                      369289 non-null  float64\n",
            " 13  sampen                   369289 non-null  float64\n",
            " 14  higuci                   369289 non-null  float64\n",
            " 15  MEAN_RR                  369289 non-null  float64\n",
            " 16  MEDIAN_RR                369289 non-null  float64\n",
            " 17  SDRR                     369289 non-null  float64\n",
            " 18  RMSSD                    369289 non-null  float64\n",
            " 19  SDSD                     369289 non-null  float64\n",
            " 20  SDRR_RMSSD               369289 non-null  float64\n",
            " 21  HR                       369289 non-null  float64\n",
            " 22  pNN25                    369289 non-null  float64\n",
            " 23  pNN50                    369289 non-null  float64\n",
            " 24  KURT                     369289 non-null  float64\n",
            " 25  SKEW                     369289 non-null  float64\n",
            " 26  MEAN_REL_RR              369289 non-null  float64\n",
            " 27  MEDIAN_REL_RR            369289 non-null  float64\n",
            " 28  SDRR_REL_RR              369289 non-null  float64\n",
            " 29  RMSSD_REL_RR             369289 non-null  float64\n",
            " 30  SDSD_REL_RR              369289 non-null  float64\n",
            " 31  SDRR_RMSSD_REL_RR        369289 non-null  float64\n",
            " 32  KURT_REL_RR              369289 non-null  float64\n",
            " 33  SKEW_REL_RR              369289 non-null  float64\n",
            " 34  condition_interruption   369289 non-null  int64  \n",
            " 35  condition_no stress      369289 non-null  int64  \n",
            " 36  condition_time pressure  369289 non-null  int64  \n",
            "dtypes: float64(34), int64(3)\n",
            "memory usage: 104.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgp93WwUj6my"
      },
      "source": [
        "# Split the  data into separate training (70%) and test (30%) sets and then standardize it to unit variance:\n",
        "X = idf\n",
        "y = idf[\"HR\"]\n",
        "\n",
        "X=X.drop('HR',axis=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsriJaujj6my",
        "outputId": "90ebb397-696f-45dc-ef7f-9bd033c4f069"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(369289, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "OIpYZWgTj6mz",
        "outputId": "7bfffbe3-508d-4af8-c471-251110bee1ff"
      },
      "source": [
        "X.head(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VLF</th>\n",
              "      <th>VLF_PCT</th>\n",
              "      <th>LF</th>\n",
              "      <th>LF_PCT</th>\n",
              "      <th>LF_NU</th>\n",
              "      <th>HF</th>\n",
              "      <th>HF_PCT</th>\n",
              "      <th>HF_NU</th>\n",
              "      <th>TP</th>\n",
              "      <th>LF_HF</th>\n",
              "      <th>HF_LF</th>\n",
              "      <th>SD1</th>\n",
              "      <th>SD2</th>\n",
              "      <th>sampen</th>\n",
              "      <th>higuci</th>\n",
              "      <th>MEAN_RR</th>\n",
              "      <th>MEDIAN_RR</th>\n",
              "      <th>SDRR</th>\n",
              "      <th>RMSSD</th>\n",
              "      <th>SDSD</th>\n",
              "      <th>SDRR_RMSSD</th>\n",
              "      <th>pNN25</th>\n",
              "      <th>pNN50</th>\n",
              "      <th>KURT</th>\n",
              "      <th>SKEW</th>\n",
              "      <th>MEAN_REL_RR</th>\n",
              "      <th>MEDIAN_REL_RR</th>\n",
              "      <th>SDRR_REL_RR</th>\n",
              "      <th>RMSSD_REL_RR</th>\n",
              "      <th>SDSD_REL_RR</th>\n",
              "      <th>SDRR_RMSSD_REL_RR</th>\n",
              "      <th>KURT_REL_RR</th>\n",
              "      <th>SKEW_REL_RR</th>\n",
              "      <th>condition_interruption</th>\n",
              "      <th>condition_no stress</th>\n",
              "      <th>condition_time pressure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2661.894136</td>\n",
              "      <td>72.203287</td>\n",
              "      <td>1009.249419</td>\n",
              "      <td>27.375666</td>\n",
              "      <td>98.485263</td>\n",
              "      <td>15.522603</td>\n",
              "      <td>0.421047</td>\n",
              "      <td>1.514737</td>\n",
              "      <td>3686.666157</td>\n",
              "      <td>65.018055</td>\n",
              "      <td>0.015380</td>\n",
              "      <td>11.001565</td>\n",
              "      <td>199.061782</td>\n",
              "      <td>2.139754</td>\n",
              "      <td>1.163485</td>\n",
              "      <td>885.157845</td>\n",
              "      <td>853.763730</td>\n",
              "      <td>140.972741</td>\n",
              "      <td>15.554505</td>\n",
              "      <td>15.553371</td>\n",
              "      <td>9.063146</td>\n",
              "      <td>11.133333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>-0.856554</td>\n",
              "      <td>0.335218</td>\n",
              "      <td>-0.000203</td>\n",
              "      <td>-0.000179</td>\n",
              "      <td>0.017080</td>\n",
              "      <td>0.007969</td>\n",
              "      <td>0.007969</td>\n",
              "      <td>2.143342</td>\n",
              "      <td>-0.856554</td>\n",
              "      <td>0.335218</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2314.265450</td>\n",
              "      <td>76.975728</td>\n",
              "      <td>690.113275</td>\n",
              "      <td>22.954139</td>\n",
              "      <td>99.695397</td>\n",
              "      <td>2.108525</td>\n",
              "      <td>0.070133</td>\n",
              "      <td>0.304603</td>\n",
              "      <td>3006.487251</td>\n",
              "      <td>327.296635</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>9.170129</td>\n",
              "      <td>114.634458</td>\n",
              "      <td>2.174499</td>\n",
              "      <td>1.084711</td>\n",
              "      <td>939.425371</td>\n",
              "      <td>948.357865</td>\n",
              "      <td>81.317742</td>\n",
              "      <td>12.964439</td>\n",
              "      <td>12.964195</td>\n",
              "      <td>6.272369</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.408190</td>\n",
              "      <td>-0.155286</td>\n",
              "      <td>-0.000059</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>0.013978</td>\n",
              "      <td>0.004769</td>\n",
              "      <td>0.004769</td>\n",
              "      <td>2.930855</td>\n",
              "      <td>-0.408190</td>\n",
              "      <td>-0.155286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1373.887112</td>\n",
              "      <td>51.152225</td>\n",
              "      <td>1298.222619</td>\n",
              "      <td>48.335104</td>\n",
              "      <td>98.950472</td>\n",
              "      <td>13.769729</td>\n",
              "      <td>0.512671</td>\n",
              "      <td>1.049528</td>\n",
              "      <td>2685.879461</td>\n",
              "      <td>94.280910</td>\n",
              "      <td>0.010607</td>\n",
              "      <td>11.533417</td>\n",
              "      <td>118.939253</td>\n",
              "      <td>2.135350</td>\n",
              "      <td>1.176315</td>\n",
              "      <td>898.186047</td>\n",
              "      <td>907.006860</td>\n",
              "      <td>84.497236</td>\n",
              "      <td>16.305279</td>\n",
              "      <td>16.305274</td>\n",
              "      <td>5.182201</td>\n",
              "      <td>13.066667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.351789</td>\n",
              "      <td>-0.656813</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.000263</td>\n",
              "      <td>0.018539</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>2.127053</td>\n",
              "      <td>0.351789</td>\n",
              "      <td>-0.656813</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2410.357408</td>\n",
              "      <td>70.180308</td>\n",
              "      <td>1005.981659</td>\n",
              "      <td>29.290305</td>\n",
              "      <td>98.224706</td>\n",
              "      <td>18.181913</td>\n",
              "      <td>0.529387</td>\n",
              "      <td>1.775294</td>\n",
              "      <td>3434.520980</td>\n",
              "      <td>55.328701</td>\n",
              "      <td>0.018074</td>\n",
              "      <td>11.119476</td>\n",
              "      <td>127.318597</td>\n",
              "      <td>2.178341</td>\n",
              "      <td>1.179688</td>\n",
              "      <td>881.757865</td>\n",
              "      <td>893.460030</td>\n",
              "      <td>90.370537</td>\n",
              "      <td>15.720468</td>\n",
              "      <td>15.720068</td>\n",
              "      <td>5.748591</td>\n",
              "      <td>11.800000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>-0.504947</td>\n",
              "      <td>-0.386138</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.017761</td>\n",
              "      <td>0.008660</td>\n",
              "      <td>0.008660</td>\n",
              "      <td>2.050988</td>\n",
              "      <td>-0.504947</td>\n",
              "      <td>-0.386138</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1151.177330</td>\n",
              "      <td>43.918366</td>\n",
              "      <td>1421.782051</td>\n",
              "      <td>54.242160</td>\n",
              "      <td>96.720007</td>\n",
              "      <td>48.215822</td>\n",
              "      <td>1.839473</td>\n",
              "      <td>3.279993</td>\n",
              "      <td>2621.175204</td>\n",
              "      <td>29.487873</td>\n",
              "      <td>0.033912</td>\n",
              "      <td>13.590641</td>\n",
              "      <td>87.718281</td>\n",
              "      <td>2.221121</td>\n",
              "      <td>1.249612</td>\n",
              "      <td>809.625331</td>\n",
              "      <td>811.184865</td>\n",
              "      <td>62.766242</td>\n",
              "      <td>19.213819</td>\n",
              "      <td>19.213657</td>\n",
              "      <td>3.266724</td>\n",
              "      <td>20.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>-0.548408</td>\n",
              "      <td>-0.154252</td>\n",
              "      <td>-0.000100</td>\n",
              "      <td>-0.002736</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>0.013055</td>\n",
              "      <td>0.013055</td>\n",
              "      <td>1.816544</td>\n",
              "      <td>-0.548408</td>\n",
              "      <td>-0.154252</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3300.245844</td>\n",
              "      <td>95.316204</td>\n",
              "      <td>151.145149</td>\n",
              "      <td>4.365306</td>\n",
              "      <td>93.200171</td>\n",
              "      <td>11.027460</td>\n",
              "      <td>0.318490</td>\n",
              "      <td>6.799829</td>\n",
              "      <td>3462.418453</td>\n",
              "      <td>13.706252</td>\n",
              "      <td>0.072959</td>\n",
              "      <td>7.026695</td>\n",
              "      <td>731.873468</td>\n",
              "      <td>0.582616</td>\n",
              "      <td>1.128483</td>\n",
              "      <td>923.283866</td>\n",
              "      <td>617.794160</td>\n",
              "      <td>517.536544</td>\n",
              "      <td>9.965976</td>\n",
              "      <td>9.933933</td>\n",
              "      <td>51.930344</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>-0.893858</td>\n",
              "      <td>1.026302</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.011061</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>1.847605</td>\n",
              "      <td>-0.893858</td>\n",
              "      <td>1.026302</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>758.674608</td>\n",
              "      <td>61.022078</td>\n",
              "      <td>483.114475</td>\n",
              "      <td>38.858094</td>\n",
              "      <td>99.692575</td>\n",
              "      <td>1.489796</td>\n",
              "      <td>0.119828</td>\n",
              "      <td>0.307425</td>\n",
              "      <td>1243.278879</td>\n",
              "      <td>324.282351</td>\n",
              "      <td>0.003084</td>\n",
              "      <td>7.528700</td>\n",
              "      <td>116.295081</td>\n",
              "      <td>2.161461</td>\n",
              "      <td>1.158004</td>\n",
              "      <td>973.252908</td>\n",
              "      <td>964.650020</td>\n",
              "      <td>82.405179</td>\n",
              "      <td>10.644196</td>\n",
              "      <td>10.643638</td>\n",
              "      <td>7.741794</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.442670</td>\n",
              "      <td>0.102908</td>\n",
              "      <td>-0.000124</td>\n",
              "      <td>-0.000583</td>\n",
              "      <td>0.010997</td>\n",
              "      <td>0.004772</td>\n",
              "      <td>0.004772</td>\n",
              "      <td>2.304640</td>\n",
              "      <td>-0.442670</td>\n",
              "      <td>0.102908</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1458.810124</td>\n",
              "      <td>75.758666</td>\n",
              "      <td>437.878087</td>\n",
              "      <td>22.739806</td>\n",
              "      <td>93.805918</td>\n",
              "      <td>28.913453</td>\n",
              "      <td>1.501528</td>\n",
              "      <td>6.194082</td>\n",
              "      <td>1925.601664</td>\n",
              "      <td>15.144441</td>\n",
              "      <td>0.066031</td>\n",
              "      <td>6.703994</td>\n",
              "      <td>185.815874</td>\n",
              "      <td>1.110739</td>\n",
              "      <td>1.146555</td>\n",
              "      <td>715.914682</td>\n",
              "      <td>679.499395</td>\n",
              "      <td>131.477151</td>\n",
              "      <td>9.477727</td>\n",
              "      <td>9.477717</td>\n",
              "      <td>13.872224</td>\n",
              "      <td>2.533333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>5.224736</td>\n",
              "      <td>2.452996</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.013206</td>\n",
              "      <td>0.006843</td>\n",
              "      <td>0.006843</td>\n",
              "      <td>1.929994</td>\n",
              "      <td>5.224736</td>\n",
              "      <td>2.452996</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2124.918400</td>\n",
              "      <td>67.479320</td>\n",
              "      <td>1003.315816</td>\n",
              "      <td>31.861491</td>\n",
              "      <td>97.973018</td>\n",
              "      <td>20.757787</td>\n",
              "      <td>0.659188</td>\n",
              "      <td>2.026982</td>\n",
              "      <td>3148.992003</td>\n",
              "      <td>48.334430</td>\n",
              "      <td>0.020689</td>\n",
              "      <td>10.349326</td>\n",
              "      <td>122.621056</td>\n",
              "      <td>2.174233</td>\n",
              "      <td>1.122471</td>\n",
              "      <td>814.257021</td>\n",
              "      <td>827.522830</td>\n",
              "      <td>87.014459</td>\n",
              "      <td>14.632232</td>\n",
              "      <td>14.631275</td>\n",
              "      <td>5.946766</td>\n",
              "      <td>7.733333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>-0.455008</td>\n",
              "      <td>-0.371959</td>\n",
              "      <td>-0.000187</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.018204</td>\n",
              "      <td>0.008242</td>\n",
              "      <td>0.008242</td>\n",
              "      <td>2.208637</td>\n",
              "      <td>-0.455008</td>\n",
              "      <td>-0.371959</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1180.987047</td>\n",
              "      <td>69.230785</td>\n",
              "      <td>522.310281</td>\n",
              "      <td>30.618414</td>\n",
              "      <td>99.509898</td>\n",
              "      <td>2.572459</td>\n",
              "      <td>0.150800</td>\n",
              "      <td>0.490102</td>\n",
              "      <td>1705.869787</td>\n",
              "      <td>203.039304</td>\n",
              "      <td>0.004925</td>\n",
              "      <td>8.498966</td>\n",
              "      <td>77.180192</td>\n",
              "      <td>2.171600</td>\n",
              "      <td>1.176054</td>\n",
              "      <td>959.694591</td>\n",
              "      <td>957.895600</td>\n",
              "      <td>54.904529</td>\n",
              "      <td>12.015400</td>\n",
              "      <td>12.015343</td>\n",
              "      <td>4.569513</td>\n",
              "      <td>3.266667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.413338</td>\n",
              "      <td>-0.018134</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.012648</td>\n",
              "      <td>0.005719</td>\n",
              "      <td>0.005719</td>\n",
              "      <td>2.211739</td>\n",
              "      <td>0.413338</td>\n",
              "      <td>-0.018134</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           VLF    VLF_PCT  ...  condition_no stress  condition_time pressure\n",
              "0  2661.894136  72.203287  ...                    1                        0\n",
              "1  2314.265450  76.975728  ...                    0                        0\n",
              "2  1373.887112  51.152225  ...                    0                        0\n",
              "3  2410.357408  70.180308  ...                    1                        0\n",
              "4  1151.177330  43.918366  ...                    1                        0\n",
              "5  3300.245844  95.316204  ...                    1                        0\n",
              "6   758.674608  61.022078  ...                    1                        0\n",
              "7  1458.810124  75.758666  ...                    1                        0\n",
              "8  2124.918400  67.479320  ...                    0                        0\n",
              "9  1180.987047  69.230785  ...                    1                        0\n",
              "\n",
              "[10 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNNjqdAWj6mz",
        "outputId": "58d1b210-9b67-4451-e87e-4ca68d275404"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(369289,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u47cQc_Vj6mz",
        "outputId": "4dc6027b-def9-4284-b13d-5f3d0cdcdeb0"
      },
      "source": [
        "y.head(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    69.499952\n",
              "1    64.363150\n",
              "2    67.450066\n",
              "3    68.809562\n",
              "4    74.565728\n",
              "5    81.342254\n",
              "6    62.095066\n",
              "7    85.857703\n",
              "8    74.588857\n",
              "9    62.726998\n",
              "Name: HR, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl6jb901j6mz"
      },
      "source": [
        "# Todo : normalize the data\n",
        "from scipy.stats import zscore\n",
        "X_z = X.apply(zscore)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3IRsO5Tj6mz"
      },
      "source": [
        "# Split data in to test and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_z, y, test_size=0.01, random_state=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqGYJdWb-3QU",
        "outputId": "4050f470-10a1-45e0-a29d-a3a7070b89f8"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(365596, 36)\n",
            "(365596,)\n",
            "(3693, 36)\n",
            "(3693,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7gxiH8T-3fO"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8e5AV45-3he",
        "outputId": "f803c95f-a48a-4aab-a438-f13e5cdc2e62"
      },
      "source": [
        "# Add drop out layer\n",
        "# Clear any previous model from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(2000, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(1500, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# drop out\n",
        "#NN_model.add(Dropout(0.25))\n",
        "NN_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# drop out\n",
        "#NN_model.add(Dropout(0.25))\n",
        "NN_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# drop out\n",
        "#NN_model.add(Dropout(0.25))\n",
        "NN_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "# drop out\n",
        "#NN_model.add(Dropout(0.25))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 2000)              74000     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1500)              3001500   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1537024   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 7,762,349\n",
            "Trainable params: 7,762,349\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFUZY5M6-3kZ"
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFfCGfli_dPC",
        "outputId": "76bdf354-dd1a-49b2-8490-282612621fed"
      },
      "source": [
        "NN_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split = 0.2)\n",
        "NN_model_1 = NN_model\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0759 - mean_absolute_error: 0.0759 - val_loss: 0.1563 - val_mean_absolute_error: 0.1563\n",
            "Epoch 2/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0756 - mean_absolute_error: 0.0756 - val_loss: 0.0701 - val_mean_absolute_error: 0.0701\n",
            "Epoch 3/200\n",
            "2285/2285 [==============================] - 10s 5ms/step - loss: 0.0782 - mean_absolute_error: 0.0782 - val_loss: 0.0592 - val_mean_absolute_error: 0.0592\n",
            "Epoch 4/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0784 - mean_absolute_error: 0.0784 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
            "Epoch 5/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0760 - mean_absolute_error: 0.0760 - val_loss: 0.1700 - val_mean_absolute_error: 0.1700\n",
            "Epoch 6/200\n",
            "2285/2285 [==============================] - 10s 5ms/step - loss: 0.0741 - mean_absolute_error: 0.0741 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
            "Epoch 7/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0742 - mean_absolute_error: 0.0742 - val_loss: 0.0550 - val_mean_absolute_error: 0.0550\n",
            "Epoch 8/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0806 - mean_absolute_error: 0.0806 - val_loss: 0.0706 - val_mean_absolute_error: 0.0706\n",
            "Epoch 9/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0743 - mean_absolute_error: 0.0743 - val_loss: 0.0342 - val_mean_absolute_error: 0.0342\n",
            "Epoch 10/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0769 - mean_absolute_error: 0.0769 - val_loss: 0.0448 - val_mean_absolute_error: 0.0448\n",
            "Epoch 11/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0747 - mean_absolute_error: 0.0747 - val_loss: 0.0515 - val_mean_absolute_error: 0.0515\n",
            "Epoch 12/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0745 - mean_absolute_error: 0.0745 - val_loss: 0.0322 - val_mean_absolute_error: 0.0322\n",
            "Epoch 13/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0768 - mean_absolute_error: 0.0768 - val_loss: 0.0601 - val_mean_absolute_error: 0.0601\n",
            "Epoch 14/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0701 - mean_absolute_error: 0.0701 - val_loss: 0.0563 - val_mean_absolute_error: 0.0563\n",
            "Epoch 15/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0771 - mean_absolute_error: 0.0771 - val_loss: 0.0335 - val_mean_absolute_error: 0.0335\n",
            "Epoch 16/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0706 - mean_absolute_error: 0.0706 - val_loss: 0.0640 - val_mean_absolute_error: 0.0640\n",
            "Epoch 17/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0779 - mean_absolute_error: 0.0779 - val_loss: 0.0591 - val_mean_absolute_error: 0.0591\n",
            "Epoch 18/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0725 - mean_absolute_error: 0.0725 - val_loss: 0.0597 - val_mean_absolute_error: 0.0597\n",
            "Epoch 19/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0727 - mean_absolute_error: 0.0727 - val_loss: 0.1436 - val_mean_absolute_error: 0.1436\n",
            "Epoch 20/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0742 - mean_absolute_error: 0.0742 - val_loss: 0.0356 - val_mean_absolute_error: 0.0356\n",
            "Epoch 21/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0728 - mean_absolute_error: 0.0728 - val_loss: 0.1012 - val_mean_absolute_error: 0.1012\n",
            "Epoch 22/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0738 - mean_absolute_error: 0.0738 - val_loss: 0.0957 - val_mean_absolute_error: 0.0957\n",
            "Epoch 23/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0701 - mean_absolute_error: 0.0701 - val_loss: 0.0978 - val_mean_absolute_error: 0.0978\n",
            "Epoch 24/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0687 - mean_absolute_error: 0.0687 - val_loss: 0.0757 - val_mean_absolute_error: 0.0757\n",
            "Epoch 25/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0700 - mean_absolute_error: 0.0700 - val_loss: 0.0844 - val_mean_absolute_error: 0.0844\n",
            "Epoch 26/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0718 - mean_absolute_error: 0.0718 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431\n",
            "Epoch 27/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0675 - mean_absolute_error: 0.0675 - val_loss: 0.0857 - val_mean_absolute_error: 0.0857\n",
            "Epoch 28/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0677 - mean_absolute_error: 0.0677 - val_loss: 0.0520 - val_mean_absolute_error: 0.0520\n",
            "Epoch 29/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0708 - mean_absolute_error: 0.0708 - val_loss: 0.0699 - val_mean_absolute_error: 0.0699\n",
            "Epoch 30/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0687 - mean_absolute_error: 0.0687 - val_loss: 0.1549 - val_mean_absolute_error: 0.1549\n",
            "Epoch 31/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0733 - mean_absolute_error: 0.0733 - val_loss: 0.0514 - val_mean_absolute_error: 0.0514\n",
            "Epoch 32/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0649 - mean_absolute_error: 0.0649 - val_loss: 0.0454 - val_mean_absolute_error: 0.0454\n",
            "Epoch 33/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0710 - mean_absolute_error: 0.0710 - val_loss: 0.2351 - val_mean_absolute_error: 0.2351\n",
            "Epoch 34/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.0497 - val_mean_absolute_error: 0.0497\n",
            "Epoch 35/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0707 - mean_absolute_error: 0.0707 - val_loss: 0.0666 - val_mean_absolute_error: 0.0666\n",
            "Epoch 36/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0744 - mean_absolute_error: 0.0744 - val_loss: 0.0919 - val_mean_absolute_error: 0.0919\n",
            "Epoch 37/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0705 - mean_absolute_error: 0.0705 - val_loss: 0.0308 - val_mean_absolute_error: 0.0308\n",
            "Epoch 38/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0717 - mean_absolute_error: 0.0717 - val_loss: 0.1049 - val_mean_absolute_error: 0.1049\n",
            "Epoch 39/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0667 - mean_absolute_error: 0.0667 - val_loss: 0.1385 - val_mean_absolute_error: 0.1385\n",
            "Epoch 40/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0716 - mean_absolute_error: 0.0716 - val_loss: 0.0750 - val_mean_absolute_error: 0.0750\n",
            "Epoch 41/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0697 - mean_absolute_error: 0.0697 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n",
            "Epoch 42/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0693 - mean_absolute_error: 0.0693 - val_loss: 0.0463 - val_mean_absolute_error: 0.0463\n",
            "Epoch 43/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0710 - mean_absolute_error: 0.0710 - val_loss: 0.0325 - val_mean_absolute_error: 0.0325\n",
            "Epoch 44/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0715 - mean_absolute_error: 0.0715 - val_loss: 0.0309 - val_mean_absolute_error: 0.0309\n",
            "Epoch 45/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.1399 - val_mean_absolute_error: 0.1399\n",
            "Epoch 46/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0680 - mean_absolute_error: 0.0680 - val_loss: 0.0350 - val_mean_absolute_error: 0.0350\n",
            "Epoch 47/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0707 - mean_absolute_error: 0.0707 - val_loss: 0.1047 - val_mean_absolute_error: 0.1047\n",
            "Epoch 48/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0704 - mean_absolute_error: 0.0704 - val_loss: 0.0358 - val_mean_absolute_error: 0.0358\n",
            "Epoch 49/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0712 - mean_absolute_error: 0.0712 - val_loss: 0.0396 - val_mean_absolute_error: 0.0396\n",
            "Epoch 50/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0709 - mean_absolute_error: 0.0709 - val_loss: 0.1147 - val_mean_absolute_error: 0.1147\n",
            "Epoch 51/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0693 - mean_absolute_error: 0.0693 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
            "Epoch 52/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0697 - mean_absolute_error: 0.0697 - val_loss: 0.1492 - val_mean_absolute_error: 0.1492\n",
            "Epoch 53/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0727 - mean_absolute_error: 0.0727 - val_loss: 0.0911 - val_mean_absolute_error: 0.0911\n",
            "Epoch 54/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0702 - mean_absolute_error: 0.0702 - val_loss: 0.0808 - val_mean_absolute_error: 0.0808\n",
            "Epoch 55/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0716 - mean_absolute_error: 0.0716 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
            "Epoch 56/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0672 - mean_absolute_error: 0.0672 - val_loss: 0.1118 - val_mean_absolute_error: 0.1118\n",
            "Epoch 57/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0671 - mean_absolute_error: 0.0671 - val_loss: 0.0572 - val_mean_absolute_error: 0.0572\n",
            "Epoch 58/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0696 - mean_absolute_error: 0.0696 - val_loss: 0.1048 - val_mean_absolute_error: 0.1048\n",
            "Epoch 59/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0656 - mean_absolute_error: 0.0656 - val_loss: 0.0513 - val_mean_absolute_error: 0.0513\n",
            "Epoch 60/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0648 - mean_absolute_error: 0.0648 - val_loss: 0.0657 - val_mean_absolute_error: 0.0657\n",
            "Epoch 61/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0674 - mean_absolute_error: 0.0674 - val_loss: 0.0905 - val_mean_absolute_error: 0.0905\n",
            "Epoch 62/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0699 - mean_absolute_error: 0.0699 - val_loss: 0.0872 - val_mean_absolute_error: 0.0872\n",
            "Epoch 63/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0690 - mean_absolute_error: 0.0690 - val_loss: 0.0506 - val_mean_absolute_error: 0.0506\n",
            "Epoch 64/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0690 - mean_absolute_error: 0.0690 - val_loss: 0.0426 - val_mean_absolute_error: 0.0426\n",
            "Epoch 65/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.0914 - val_mean_absolute_error: 0.0914\n",
            "Epoch 66/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0656 - mean_absolute_error: 0.0656 - val_loss: 0.0612 - val_mean_absolute_error: 0.0612\n",
            "Epoch 67/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0652 - mean_absolute_error: 0.0652 - val_loss: 0.0575 - val_mean_absolute_error: 0.0575\n",
            "Epoch 68/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0707 - mean_absolute_error: 0.0707 - val_loss: 0.0827 - val_mean_absolute_error: 0.0827\n",
            "Epoch 69/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0689 - mean_absolute_error: 0.0689 - val_loss: 0.0513 - val_mean_absolute_error: 0.0513\n",
            "Epoch 70/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0640 - mean_absolute_error: 0.0640 - val_loss: 0.0296 - val_mean_absolute_error: 0.0296\n",
            "Epoch 71/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0668 - mean_absolute_error: 0.0668 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
            "Epoch 72/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0654 - mean_absolute_error: 0.0654 - val_loss: 0.0262 - val_mean_absolute_error: 0.0262\n",
            "Epoch 73/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0684 - mean_absolute_error: 0.0684 - val_loss: 0.0623 - val_mean_absolute_error: 0.0623\n",
            "Epoch 74/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0673 - mean_absolute_error: 0.0673 - val_loss: 0.1326 - val_mean_absolute_error: 0.1326\n",
            "Epoch 75/200\n",
            "2285/2285 [==============================] - 12s 5ms/step - loss: 0.0657 - mean_absolute_error: 0.0657 - val_loss: 0.0883 - val_mean_absolute_error: 0.0883\n",
            "Epoch 76/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0675 - mean_absolute_error: 0.0675 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
            "Epoch 77/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0655 - mean_absolute_error: 0.0655 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
            "Epoch 78/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0689 - mean_absolute_error: 0.0689 - val_loss: 0.0720 - val_mean_absolute_error: 0.0720\n",
            "Epoch 79/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0682 - mean_absolute_error: 0.0682 - val_loss: 0.0296 - val_mean_absolute_error: 0.0296\n",
            "Epoch 80/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0647 - mean_absolute_error: 0.0647 - val_loss: 0.0364 - val_mean_absolute_error: 0.0364\n",
            "Epoch 81/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0669 - mean_absolute_error: 0.0669 - val_loss: 0.0644 - val_mean_absolute_error: 0.0644\n",
            "Epoch 82/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0649 - mean_absolute_error: 0.0649 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466\n",
            "Epoch 83/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0635 - mean_absolute_error: 0.0635 - val_loss: 0.0999 - val_mean_absolute_error: 0.0999\n",
            "Epoch 84/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0668 - mean_absolute_error: 0.0668 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
            "Epoch 85/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0659 - mean_absolute_error: 0.0659 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
            "Epoch 86/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0646 - mean_absolute_error: 0.0646 - val_loss: 0.0294 - val_mean_absolute_error: 0.0294\n",
            "Epoch 87/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0658 - mean_absolute_error: 0.0658 - val_loss: 0.0346 - val_mean_absolute_error: 0.0346\n",
            "Epoch 88/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0644 - mean_absolute_error: 0.0644 - val_loss: 0.0692 - val_mean_absolute_error: 0.0692\n",
            "Epoch 89/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0697 - mean_absolute_error: 0.0697 - val_loss: 0.0391 - val_mean_absolute_error: 0.0391\n",
            "Epoch 90/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0635 - mean_absolute_error: 0.0635 - val_loss: 0.0280 - val_mean_absolute_error: 0.0280\n",
            "Epoch 91/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0660 - mean_absolute_error: 0.0660 - val_loss: 0.0235 - val_mean_absolute_error: 0.0235\n",
            "Epoch 92/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0646 - mean_absolute_error: 0.0646 - val_loss: 0.0520 - val_mean_absolute_error: 0.0520\n",
            "Epoch 93/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0591 - mean_absolute_error: 0.0591 - val_loss: 0.0292 - val_mean_absolute_error: 0.0292\n",
            "Epoch 94/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0662 - mean_absolute_error: 0.0662 - val_loss: 0.0279 - val_mean_absolute_error: 0.0279\n",
            "Epoch 95/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0648 - mean_absolute_error: 0.0648 - val_loss: 0.1390 - val_mean_absolute_error: 0.1390\n",
            "Epoch 96/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0662 - mean_absolute_error: 0.0662 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307\n",
            "Epoch 97/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0629 - mean_absolute_error: 0.0629 - val_loss: 0.0608 - val_mean_absolute_error: 0.0608\n",
            "Epoch 98/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0633 - mean_absolute_error: 0.0633 - val_loss: 0.0478 - val_mean_absolute_error: 0.0478\n",
            "Epoch 99/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0656 - mean_absolute_error: 0.0656 - val_loss: 0.0254 - val_mean_absolute_error: 0.0254\n",
            "Epoch 100/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0671 - mean_absolute_error: 0.0671 - val_loss: 0.0920 - val_mean_absolute_error: 0.0920\n",
            "Epoch 101/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0651 - mean_absolute_error: 0.0651 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332\n",
            "Epoch 102/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0685 - mean_absolute_error: 0.0685 - val_loss: 0.0539 - val_mean_absolute_error: 0.0539\n",
            "Epoch 103/200\n",
            "2285/2285 [==============================] - 12s 5ms/step - loss: 0.0606 - mean_absolute_error: 0.0606 - val_loss: 0.0807 - val_mean_absolute_error: 0.0807\n",
            "Epoch 104/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0639 - mean_absolute_error: 0.0639 - val_loss: 0.0363 - val_mean_absolute_error: 0.0363\n",
            "Epoch 105/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0611 - mean_absolute_error: 0.0611 - val_loss: 0.0739 - val_mean_absolute_error: 0.0739\n",
            "Epoch 106/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0664 - mean_absolute_error: 0.0664 - val_loss: 0.0534 - val_mean_absolute_error: 0.0534\n",
            "Epoch 107/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0673 - mean_absolute_error: 0.0673 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431\n",
            "Epoch 108/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0667 - mean_absolute_error: 0.0667 - val_loss: 0.0799 - val_mean_absolute_error: 0.0799\n",
            "Epoch 109/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0653 - mean_absolute_error: 0.0653 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
            "Epoch 110/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0609 - mean_absolute_error: 0.0609 - val_loss: 0.0477 - val_mean_absolute_error: 0.0477\n",
            "Epoch 111/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0652 - mean_absolute_error: 0.0652 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
            "Epoch 112/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0601 - mean_absolute_error: 0.0601 - val_loss: 0.0262 - val_mean_absolute_error: 0.0262\n",
            "Epoch 113/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0610 - mean_absolute_error: 0.0610 - val_loss: 0.0922 - val_mean_absolute_error: 0.0922\n",
            "Epoch 114/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0635 - mean_absolute_error: 0.0635 - val_loss: 0.0251 - val_mean_absolute_error: 0.0251\n",
            "Epoch 115/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.0934 - val_mean_absolute_error: 0.0934\n",
            "Epoch 116/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.0512 - val_mean_absolute_error: 0.0512\n",
            "Epoch 117/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0647 - mean_absolute_error: 0.0647 - val_loss: 0.1137 - val_mean_absolute_error: 0.1137\n",
            "Epoch 118/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0643 - mean_absolute_error: 0.0643 - val_loss: 0.0552 - val_mean_absolute_error: 0.0552\n",
            "Epoch 119/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0659 - mean_absolute_error: 0.0659 - val_loss: 0.0276 - val_mean_absolute_error: 0.0276\n",
            "Epoch 120/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0614 - mean_absolute_error: 0.0614 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
            "Epoch 121/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0619 - mean_absolute_error: 0.0619 - val_loss: 0.0351 - val_mean_absolute_error: 0.0351\n",
            "Epoch 122/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0651 - mean_absolute_error: 0.0651 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
            "Epoch 123/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0669 - mean_absolute_error: 0.0669 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417\n",
            "Epoch 124/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0665 - mean_absolute_error: 0.0665 - val_loss: 0.0266 - val_mean_absolute_error: 0.0266\n",
            "Epoch 125/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0616 - mean_absolute_error: 0.0616 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
            "Epoch 126/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0620 - mean_absolute_error: 0.0620 - val_loss: 0.0274 - val_mean_absolute_error: 0.0274\n",
            "Epoch 127/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0620 - mean_absolute_error: 0.0620 - val_loss: 0.0953 - val_mean_absolute_error: 0.0953\n",
            "Epoch 128/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0607 - mean_absolute_error: 0.0607 - val_loss: 0.0787 - val_mean_absolute_error: 0.0787\n",
            "Epoch 129/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0628 - mean_absolute_error: 0.0628 - val_loss: 0.0996 - val_mean_absolute_error: 0.0996\n",
            "Epoch 130/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0625 - mean_absolute_error: 0.0625 - val_loss: 0.0771 - val_mean_absolute_error: 0.0771\n",
            "Epoch 131/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0615 - mean_absolute_error: 0.0615 - val_loss: 0.0769 - val_mean_absolute_error: 0.0769\n",
            "Epoch 132/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0632 - mean_absolute_error: 0.0632 - val_loss: 0.0949 - val_mean_absolute_error: 0.0949\n",
            "Epoch 133/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0613 - mean_absolute_error: 0.0613 - val_loss: 0.0309 - val_mean_absolute_error: 0.0309\n",
            "Epoch 134/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0618 - mean_absolute_error: 0.0618 - val_loss: 0.0807 - val_mean_absolute_error: 0.0807\n",
            "Epoch 135/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0606 - mean_absolute_error: 0.0606 - val_loss: 0.0478 - val_mean_absolute_error: 0.0478\n",
            "Epoch 136/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0661 - mean_absolute_error: 0.0661 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304\n",
            "Epoch 137/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0620 - mean_absolute_error: 0.0620 - val_loss: 0.0784 - val_mean_absolute_error: 0.0784\n",
            "Epoch 138/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0627 - mean_absolute_error: 0.0627 - val_loss: 0.0328 - val_mean_absolute_error: 0.0328\n",
            "Epoch 139/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0614 - mean_absolute_error: 0.0614 - val_loss: 0.0326 - val_mean_absolute_error: 0.0326\n",
            "Epoch 140/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0586 - mean_absolute_error: 0.0586 - val_loss: 0.0432 - val_mean_absolute_error: 0.0432\n",
            "Epoch 141/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0595 - mean_absolute_error: 0.0595 - val_loss: 0.1011 - val_mean_absolute_error: 0.1011\n",
            "Epoch 142/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0615 - mean_absolute_error: 0.0615 - val_loss: 0.0338 - val_mean_absolute_error: 0.0338\n",
            "Epoch 143/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.0361 - val_mean_absolute_error: 0.0361\n",
            "Epoch 144/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0657 - mean_absolute_error: 0.0657 - val_loss: 0.0478 - val_mean_absolute_error: 0.0478\n",
            "Epoch 145/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0600 - mean_absolute_error: 0.0600 - val_loss: 0.1086 - val_mean_absolute_error: 0.1086\n",
            "Epoch 146/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0603 - mean_absolute_error: 0.0603 - val_loss: 0.0252 - val_mean_absolute_error: 0.0252\n",
            "Epoch 147/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0633 - mean_absolute_error: 0.0633 - val_loss: 0.1676 - val_mean_absolute_error: 0.1676\n",
            "Epoch 148/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0597 - mean_absolute_error: 0.0597 - val_loss: 0.0586 - val_mean_absolute_error: 0.0586\n",
            "Epoch 149/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0619 - mean_absolute_error: 0.0619 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
            "Epoch 150/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0595 - mean_absolute_error: 0.0595 - val_loss: 0.0900 - val_mean_absolute_error: 0.0900\n",
            "Epoch 151/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0635 - mean_absolute_error: 0.0635 - val_loss: 0.0616 - val_mean_absolute_error: 0.0616\n",
            "Epoch 152/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.0866 - val_mean_absolute_error: 0.0866\n",
            "Epoch 153/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0625 - mean_absolute_error: 0.0625 - val_loss: 0.0357 - val_mean_absolute_error: 0.0357\n",
            "Epoch 154/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0580 - mean_absolute_error: 0.0580 - val_loss: 0.1168 - val_mean_absolute_error: 0.1168\n",
            "Epoch 155/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0588 - mean_absolute_error: 0.0588 - val_loss: 0.0729 - val_mean_absolute_error: 0.0729\n",
            "Epoch 156/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0609 - mean_absolute_error: 0.0609 - val_loss: 0.0608 - val_mean_absolute_error: 0.0608\n",
            "Epoch 157/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0604 - mean_absolute_error: 0.0604 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
            "Epoch 158/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0590 - mean_absolute_error: 0.0590 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307\n",
            "Epoch 159/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0604 - mean_absolute_error: 0.0604 - val_loss: 0.0606 - val_mean_absolute_error: 0.0606\n",
            "Epoch 160/200\n",
            "2285/2285 [==============================] - 12s 5ms/step - loss: 0.0585 - mean_absolute_error: 0.0585 - val_loss: 0.0362 - val_mean_absolute_error: 0.0362\n",
            "Epoch 161/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0613 - mean_absolute_error: 0.0613 - val_loss: 0.0995 - val_mean_absolute_error: 0.0995\n",
            "Epoch 162/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0567 - mean_absolute_error: 0.0567 - val_loss: 0.0339 - val_mean_absolute_error: 0.0339\n",
            "Epoch 163/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0552 - mean_absolute_error: 0.0552 - val_loss: 0.0440 - val_mean_absolute_error: 0.0440\n",
            "Epoch 164/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0585 - mean_absolute_error: 0.0585 - val_loss: 0.0223 - val_mean_absolute_error: 0.0223\n",
            "Epoch 165/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0568 - mean_absolute_error: 0.0568 - val_loss: 0.0262 - val_mean_absolute_error: 0.0262\n",
            "Epoch 166/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0590 - mean_absolute_error: 0.0590 - val_loss: 0.0227 - val_mean_absolute_error: 0.0227\n",
            "Epoch 167/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0582 - mean_absolute_error: 0.0582 - val_loss: 0.1288 - val_mean_absolute_error: 0.1288\n",
            "Epoch 168/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0603 - mean_absolute_error: 0.0603 - val_loss: 0.0550 - val_mean_absolute_error: 0.0550\n",
            "Epoch 169/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0593 - mean_absolute_error: 0.0593 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304\n",
            "Epoch 170/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0546 - mean_absolute_error: 0.0546 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300\n",
            "Epoch 171/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0577 - mean_absolute_error: 0.0577 - val_loss: 0.0977 - val_mean_absolute_error: 0.0977\n",
            "Epoch 172/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.0549 - val_mean_absolute_error: 0.0549\n",
            "Epoch 173/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0556 - mean_absolute_error: 0.0556 - val_loss: 0.0685 - val_mean_absolute_error: 0.0685\n",
            "Epoch 174/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0597 - mean_absolute_error: 0.0597 - val_loss: 0.0365 - val_mean_absolute_error: 0.0365\n",
            "Epoch 175/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0549 - mean_absolute_error: 0.0549 - val_loss: 0.0355 - val_mean_absolute_error: 0.0355\n",
            "Epoch 176/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0569 - mean_absolute_error: 0.0569 - val_loss: 0.1175 - val_mean_absolute_error: 0.1175\n",
            "Epoch 177/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0600 - mean_absolute_error: 0.0600 - val_loss: 0.0434 - val_mean_absolute_error: 0.0434\n",
            "Epoch 178/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0336 - val_mean_absolute_error: 0.0336\n",
            "Epoch 179/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0582 - mean_absolute_error: 0.0582 - val_loss: 0.0517 - val_mean_absolute_error: 0.0517\n",
            "Epoch 180/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0534 - mean_absolute_error: 0.0534 - val_loss: 0.0680 - val_mean_absolute_error: 0.0680\n",
            "Epoch 181/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0596 - mean_absolute_error: 0.0596 - val_loss: 0.0292 - val_mean_absolute_error: 0.0292\n",
            "Epoch 182/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0569 - mean_absolute_error: 0.0569 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
            "Epoch 183/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0582 - mean_absolute_error: 0.0582 - val_loss: 0.0423 - val_mean_absolute_error: 0.0423\n",
            "Epoch 184/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0579 - mean_absolute_error: 0.0579 - val_loss: 0.0492 - val_mean_absolute_error: 0.0492\n",
            "Epoch 185/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0588 - mean_absolute_error: 0.0588 - val_loss: 0.0353 - val_mean_absolute_error: 0.0353\n",
            "Epoch 186/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0537 - mean_absolute_error: 0.0537 - val_loss: 0.0529 - val_mean_absolute_error: 0.0529\n",
            "Epoch 187/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0549 - mean_absolute_error: 0.0549 - val_loss: 0.0513 - val_mean_absolute_error: 0.0513\n",
            "Epoch 188/200\n",
            "2285/2285 [==============================] - 12s 5ms/step - loss: 0.0565 - mean_absolute_error: 0.0565 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394\n",
            "Epoch 189/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0563 - mean_absolute_error: 0.0563 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256\n",
            "Epoch 190/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0563 - mean_absolute_error: 0.0563 - val_loss: 0.0527 - val_mean_absolute_error: 0.0527\n",
            "Epoch 191/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0553 - mean_absolute_error: 0.0553 - val_loss: 0.0498 - val_mean_absolute_error: 0.0498\n",
            "Epoch 192/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0544 - mean_absolute_error: 0.0544 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
            "Epoch 193/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0573 - mean_absolute_error: 0.0573 - val_loss: 0.0284 - val_mean_absolute_error: 0.0284\n",
            "Epoch 194/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0558 - mean_absolute_error: 0.0558 - val_loss: 0.0353 - val_mean_absolute_error: 0.0353\n",
            "Epoch 195/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0544 - mean_absolute_error: 0.0544 - val_loss: 0.0511 - val_mean_absolute_error: 0.0511\n",
            "Epoch 196/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0578 - mean_absolute_error: 0.0578 - val_loss: 0.0451 - val_mean_absolute_error: 0.0451\n",
            "Epoch 197/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0568 - mean_absolute_error: 0.0568 - val_loss: 0.0493 - val_mean_absolute_error: 0.0493\n",
            "Epoch 198/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0546 - mean_absolute_error: 0.0546 - val_loss: 0.0840 - val_mean_absolute_error: 0.0840\n",
            "Epoch 199/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0516 - mean_absolute_error: 0.0516 - val_loss: 0.0308 - val_mean_absolute_error: 0.0308\n",
            "Epoch 200/200\n",
            "2285/2285 [==============================] - 11s 5ms/step - loss: 0.0540 - mean_absolute_error: 0.0540 - val_loss: 0.0510 - val_mean_absolute_error: 0.0510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E89J47wO_dTH"
      },
      "source": [
        "#!pip install pickle\n",
        "# Save this model\n",
        "#import pickle\n",
        "#fname = 'NN_model_1.sav'\n",
        "#pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "# load the model from disk\n",
        "#mymodel = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT9RKgwd_dZB",
        "outputId": "28457a84-a0a7-4971-bd42-f5e7efe68f6c"
      },
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eqS3TwiuvyL",
        "outputId": "edbc9299-f59f-4fca-ba32-fc3a7a10f83d"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "NN_model_1.save('saved_model/NN_model_1') \n",
        "NN_model.save('saved_model/NN_model') "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: saved_model/NN_model_1/assets\n",
            "INFO:tensorflow:Assets written to: saved_model/NN_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AAVLxlkvC4c"
      },
      "source": [
        "# TEst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuIxtDHIxrMK",
        "outputId": "df926d0c-d51c-4cf9-b42a-726663e19f5a"
      },
      "source": [
        "# load test data\n",
        "orig_test_df = pd.read_csv(\"merged_orig_test_data.csv\")\n",
        "print(orig_test_df.shape)\n",
        "print(orig_test_df.info())\n",
        "print(orig_test_df.head(10))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41033, 36)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41033 entries, 0 to 41032\n",
            "Data columns (total 36 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   uuid               41033 non-null  object \n",
            " 1   VLF                41033 non-null  float64\n",
            " 2   VLF_PCT            41033 non-null  float64\n",
            " 3   LF                 41033 non-null  float64\n",
            " 4   LF_PCT             41033 non-null  float64\n",
            " 5   LF_NU              41033 non-null  float64\n",
            " 6   HF                 41033 non-null  float64\n",
            " 7   HF_PCT             41033 non-null  float64\n",
            " 8   HF_NU              41033 non-null  float64\n",
            " 9   TP                 41033 non-null  float64\n",
            " 10  LF_HF              41033 non-null  float64\n",
            " 11  HF_LF              41033 non-null  float64\n",
            " 12  SD1                41033 non-null  float64\n",
            " 13  SD2                41033 non-null  float64\n",
            " 14  sampen             41033 non-null  float64\n",
            " 15  higuci             41033 non-null  float64\n",
            " 16  datasetId          41033 non-null  int64  \n",
            " 17  condition          41033 non-null  object \n",
            " 18  MEAN_RR            41033 non-null  float64\n",
            " 19  MEDIAN_RR          41033 non-null  float64\n",
            " 20  SDRR               41033 non-null  float64\n",
            " 21  RMSSD              41033 non-null  float64\n",
            " 22  SDSD               41033 non-null  float64\n",
            " 23  SDRR_RMSSD         41033 non-null  float64\n",
            " 24  pNN25              41033 non-null  float64\n",
            " 25  pNN50              41033 non-null  float64\n",
            " 26  KURT               41033 non-null  float64\n",
            " 27  SKEW               41033 non-null  float64\n",
            " 28  MEAN_REL_RR        41033 non-null  float64\n",
            " 29  MEDIAN_REL_RR      41033 non-null  float64\n",
            " 30  SDRR_REL_RR        41033 non-null  float64\n",
            " 31  RMSSD_REL_RR       41033 non-null  float64\n",
            " 32  SDSD_REL_RR        41033 non-null  float64\n",
            " 33  SDRR_RMSSD_REL_RR  41033 non-null  float64\n",
            " 34  KURT_REL_RR        41033 non-null  float64\n",
            " 35  SKEW_REL_RR        41033 non-null  float64\n",
            "dtypes: float64(33), int64(1), object(2)\n",
            "memory usage: 11.3+ MB\n",
            "None\n",
            "                                   uuid          VLF  ...  KURT_REL_RR  SKEW_REL_RR\n",
            "0  62b75db5-bc40-4c8f-9166-daf0efcab4c2  1868.532278  ...    -0.680262    -0.233075\n",
            "1  a99549ad-3eb6-4413-bc90-9053e7f7e684   568.742845  ...    -0.034454    -0.051689\n",
            "2  cb573d3a-c767-4556-b32e-ad8c08ded214  2101.871207  ...    -0.206953    -0.589940\n",
            "3  47a0c6de-2aef-4ac3-997d-252fa6fd07f1  5757.544433  ...    -0.820407     0.487198\n",
            "4  de3fd54f-c74e-4fe8-bf2a-7a127f68b312   964.696325  ...     1.738453    -0.005082\n",
            "5  ca11d7ce-01f3-4819-926e-6a4b6a7eb5b5   960.708344  ...     0.384591    -0.387528\n",
            "6  1a3bd5ba-0190-4ebc-86a9-3c3984f17627  1275.417613  ...    -0.568992     0.060358\n",
            "7  b48d3523-1ddc-4902-954f-7593cdabf96b  4649.301851  ...    -1.610635    -0.476072\n",
            "8  e1916dc1-e0c0-4c8a-9ee9-17902c306d45  1214.157540  ...    -0.344596     0.055242\n",
            "9  189bf031-4e58-415e-af4e-981cc63afbbf  1220.239457  ...     0.924790     0.059163\n",
            "\n",
            "[10 rows x 36 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvPejqYFj6mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbe9b17-abe7-4297-8d18-b93c012f072d"
      },
      "source": [
        "# load test data\n",
        "tdf = pd.read_csv(\"tdf.csv\")  \n",
        "print(tdf.shape)\n",
        "print(tdf.info())\n",
        "print(tdf.head(10))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41033, 36)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41033 entries, 0 to 41032\n",
            "Data columns (total 36 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   VLF                      41033 non-null  float64\n",
            " 1   VLF_PCT                  41033 non-null  float64\n",
            " 2   LF                       41033 non-null  float64\n",
            " 3   LF_PCT                   41033 non-null  float64\n",
            " 4   LF_NU                    41033 non-null  float64\n",
            " 5   HF                       41033 non-null  float64\n",
            " 6   HF_PCT                   41033 non-null  float64\n",
            " 7   HF_NU                    41033 non-null  float64\n",
            " 8   TP                       41033 non-null  float64\n",
            " 9   LF_HF                    41033 non-null  float64\n",
            " 10  HF_LF                    41033 non-null  float64\n",
            " 11  SD1                      41033 non-null  float64\n",
            " 12  SD2                      41033 non-null  float64\n",
            " 13  sampen                   41033 non-null  float64\n",
            " 14  higuci                   41033 non-null  float64\n",
            " 15  MEAN_RR                  41033 non-null  float64\n",
            " 16  MEDIAN_RR                41033 non-null  float64\n",
            " 17  SDRR                     41033 non-null  float64\n",
            " 18  RMSSD                    41033 non-null  float64\n",
            " 19  SDSD                     41033 non-null  float64\n",
            " 20  SDRR_RMSSD               41033 non-null  float64\n",
            " 21  pNN25                    41033 non-null  float64\n",
            " 22  pNN50                    41033 non-null  float64\n",
            " 23  KURT                     41033 non-null  float64\n",
            " 24  SKEW                     41033 non-null  float64\n",
            " 25  MEAN_REL_RR              41033 non-null  float64\n",
            " 26  MEDIAN_REL_RR            41033 non-null  float64\n",
            " 27  SDRR_REL_RR              41033 non-null  float64\n",
            " 28  RMSSD_REL_RR             41033 non-null  float64\n",
            " 29  SDSD_REL_RR              41033 non-null  float64\n",
            " 30  SDRR_RMSSD_REL_RR        41033 non-null  float64\n",
            " 31  KURT_REL_RR              41033 non-null  float64\n",
            " 32  SKEW_REL_RR              41033 non-null  float64\n",
            " 33  condition_interruption   41033 non-null  int64  \n",
            " 34  condition_no stress      41033 non-null  int64  \n",
            " 35  condition_time pressure  41033 non-null  int64  \n",
            "dtypes: float64(33), int64(3)\n",
            "memory usage: 11.3 MB\n",
            "None\n",
            "           VLF    VLF_PCT  ...  condition_no stress  condition_time pressure\n",
            "0  1868.532278  76.511189  ...                    0                        1\n",
            "1   568.742845  26.301350  ...                    1                        0\n",
            "2  2101.871207  75.836461  ...                    1                        0\n",
            "3  5757.544433  90.562305  ...                    0                        1\n",
            "4   964.696325  70.256575  ...                    0                        0\n",
            "5   960.708344  68.701038  ...                    0                        0\n",
            "6  1275.417613  29.737139  ...                    0                        0\n",
            "7  4649.301851  97.306576  ...                    1                        0\n",
            "8  1214.157540  75.223732  ...                    1                        0\n",
            "9  1220.239457  74.826775  ...                    0                        0\n",
            "\n",
            "[10 rows x 36 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOHEeKV_vgBe"
      },
      "source": [
        "test_z = tdf.apply(zscore)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcERcZx5j6mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43bb419-4506-4ffe-f979-c1077ec786fa"
      },
      "source": [
        "print(test_z.head(10))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        VLF   VLF_PCT  ...  condition_no stress  condition_time pressure\n",
            "0 -0.180651  0.729908  ...            -1.083482                 2.187464\n",
            "1 -0.896694 -2.252038  ...             0.922950                -0.457150\n",
            "2 -0.052107  0.689836  ...             0.922950                -0.457150\n",
            "3  1.961771  1.564399  ...            -1.083482                 2.187464\n",
            "4 -0.678566  0.358449  ...            -1.083482                -0.457150\n",
            "5 -0.680763  0.266066  ...            -1.083482                -0.457150\n",
            "6 -0.507393 -2.047988  ...            -1.083482                -0.457150\n",
            "7  1.351250  1.964940  ...             0.922950                -0.457150\n",
            "8 -0.541140  0.653446  ...             0.922950                -0.457150\n",
            "9 -0.537790  0.629871  ...            -1.083482                -0.457150\n",
            "\n",
            "[10 rows x 36 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqFafemWj6mz"
      },
      "source": [
        "# predict the HR \n",
        "test_pred = NN_model.predict(test_z)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxF9dGTbj6mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6521ceec-d5e7-464f-8777-19d2a4c2207c"
      },
      "source": [
        "test_pred"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[64.75879 ],\n",
              "       [73.86613 ],\n",
              "       [69.292656],\n",
              "       ...,\n",
              "       [61.01204 ],\n",
              "       [77.41045 ],\n",
              "       [76.65783 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK1DpHznw6Mn",
        "outputId": "9b4577bc-be9c-493a-cf56-54c4dfe049de"
      },
      "source": [
        "test_pred[:,0]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([64.75879 , 73.86613 , 69.292656, ..., 61.01204 , 77.41045 ,\n",
              "       76.65783 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRy3_XYdyrmE",
        "outputId": "010e11ad-9be8-420b-c128-7830d3564b16"
      },
      "source": [
        "orig_test_df['uuid']"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        62b75db5-bc40-4c8f-9166-daf0efcab4c2\n",
              "1        a99549ad-3eb6-4413-bc90-9053e7f7e684\n",
              "2        cb573d3a-c767-4556-b32e-ad8c08ded214\n",
              "3        47a0c6de-2aef-4ac3-997d-252fa6fd07f1\n",
              "4        de3fd54f-c74e-4fe8-bf2a-7a127f68b312\n",
              "                         ...                 \n",
              "41028    11253232-cf0a-4c40-abfb-ac2795effd9b\n",
              "41029    38052c36-d08d-4305-905b-830f600ce2b9\n",
              "41030    8ef3781b-512f-42ce-bcfd-47f5c1070bb0\n",
              "41031    2bf689f3-af22-426d-91cf-e9e173ad3a54\n",
              "41032    0b654db0-e4d3-4ba3-b9ce-9c9291c8271e\n",
              "Name: uuid, Length: 41033, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pc1_xIhxNTt"
      },
      "source": [
        "result_df= pd.DataFrame({'uuid': orig_test_df['uuid'],   'HR': test_pred[:,0] })"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaWtP2HLj6mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d58fe2-fe09-4a79-f317-9c8ad279a6b0"
      },
      "source": [
        "print(result_df.head(10))\n",
        "print(result_df.shape)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                   uuid         HR\n",
            "0  62b75db5-bc40-4c8f-9166-daf0efcab4c2  64.758789\n",
            "1  a99549ad-3eb6-4413-bc90-9053e7f7e684  73.866127\n",
            "2  cb573d3a-c767-4556-b32e-ad8c08ded214  69.292656\n",
            "3  47a0c6de-2aef-4ac3-997d-252fa6fd07f1  60.295113\n",
            "4  de3fd54f-c74e-4fe8-bf2a-7a127f68b312  77.853737\n",
            "5  ca11d7ce-01f3-4819-926e-6a4b6a7eb5b5  78.686081\n",
            "6  1a3bd5ba-0190-4ebc-86a9-3c3984f17627  80.716675\n",
            "7  b48d3523-1ddc-4902-954f-7593cdabf96b  57.759514\n",
            "8  e1916dc1-e0c0-4c8a-9ee9-17902c306d45  82.326698\n",
            "9  189bf031-4e58-415e-af4e-981cc63afbbf  79.547676\n",
            "(41033, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XC2drgaxcYe"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_results\n",
        "\n",
        "# Save the result df as csv\n",
        "result_df.to_csv('saved_results/NN_model_1_results.csv', index=False)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO4MdaqCj6mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49310d4d-9772-466c-e604-2a6d6838889e"
      },
      "source": [
        "!ls -ltr saved_model/\n",
        "!ls -ltr saved_results/\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2091\n",
            "drwx------ 4 root root    4096 Dec  5 11:49 NN_model_1\n",
            "drwx------ 4 root root    4096 Dec  5 11:49 NN_model\n",
            "-rw------- 1 root root 2132429 Dec  5 12:08 NN_model_1_results.csv\n",
            "total 1853\n",
            "-rw------- 1 root root 1897340 Dec  5 12:15 NN_model_1_results.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deum1YvGj6mz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SPc0Pbtj6mz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLOjdmbhj6m0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}